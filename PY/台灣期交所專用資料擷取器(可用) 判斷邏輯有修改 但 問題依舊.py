# æª”å: å°ç£æœŸäº¤æ‰€å°ˆç”¨è³‡æ–™æ“·å–å™¨(å¯ç”¨) åˆ¤æ–·é‚è¼¯æœ‰ä¿®æ”¹ ä½† å•é¡Œä¾èˆŠ.py
import tkinter as tk
from tkinter import ttk, messagebox, scrolledtext, simpledialog, filedialog
import requests
from bs4 import BeautifulSoup
import pandas as pd
import sqlite3
import os
import re
import logging
from datetime import datetime, timedelta
import json
import csv
import threading
from urllib.parse import urljoin, urlparse
import yfinance as yf
from typing import Dict, List, Any, Optional

# å‰µå»ºDataè³‡æ–™å¤¾
if not os.path.exists('Data'):
    os.makedirs('Data')

class FinancialDatabase:
    """é‡‘èè³‡æ–™åº«ç®¡ç†ç³»çµ±"""
    
    def __init__(self, db_path: str = None):
        # ä½¿ç”¨åŸºæ–¼Pythonè…³æœ¬ä½ç½®çš„çµ•å°è·¯å¾‘
        if db_path is None:
            base_dir = os.path.dirname(os.path.abspath(__file__))
            self.db_path = os.path.join(base_dir, "Data", "financial.db")
        else:
            self.db_path = db_path
            
        self._ensure_data_directory()
        self._init_database()
        
    def _ensure_data_directory(self):
        """ç¢ºä¿Dataè³‡æ–™å¤¾å­˜åœ¨"""
        data_dir = os.path.dirname(self.db_path)
        if not os.path.exists(data_dir):
            os.makedirs(data_dir)
    
    def _init_database(self):
        """åˆå§‹åŒ–è³‡æ–™åº«è¡¨æ ¼"""
        conn = self._get_connection()
        cursor = conn.cursor()
        
        # å»ºç«‹é¸æ“‡æ¬Šè¡¨æ ¼
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS options_raw (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                product TEXT NOT NULL,
                trade_date DATE NOT NULL,
                expiry TEXT NOT NULL,
                strike REAL NOT NULL,
                cp TEXT NOT NULL CHECK (cp IN ('C', 'P')),
                volume INTEGER DEFAULT 0,
                oi INTEGER,
                raw_oi_text TEXT,
                session TEXT DEFAULT 'regular' CHECK (session IN ('regular', 'after_hours')),
                load_file TEXT,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(trade_date, product, expiry, strike, cp, session)
            )
        ''')
        
        # å»ºç«‹æœŸè²¨è¡¨æ ¼
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS futures_raw (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                product TEXT NOT NULL,
                trade_date DATE NOT NULL,
                expiry TEXT NOT NULL,
                open REAL,
                high REAL,
                low REAL,
                close REAL,
                volume INTEGER DEFAULT 0,
                oi INTEGER DEFAULT 0,
                settlement REAL,
                session TEXT DEFAULT 'regular' CHECK (session IN ('regular', 'after_hours')),
                load_file TEXT,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(trade_date, product, expiry, session)
            )
        ''')
        
        # å»ºç«‹è‚¡ç¥¨è¡¨æ ¼ï¼ˆæ–°å¢chinese_nameæ¬„ä½ï¼‰
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS stocks_raw (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                symbol TEXT NOT NULL,
                chinese_name TEXT,
                trade_date DATE NOT NULL,
                open REAL,
                high REAL,
                low REAL,
                close REAL,
                volume INTEGER DEFAULT 0,
                value REAL DEFAULT 0,
                load_file TEXT,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(trade_date, symbol)
            )
        ''')
        
        # å»ºç«‹ç´¢å¼•
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_options_trade_date ON options_raw(trade_date)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_options_product ON options_raw(product)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_options_expiry ON options_raw(expiry)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_options_strike ON options_raw(strike)')
        
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_futures_trade_date ON futures_raw(trade_date)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_futures_product ON futures_raw(product)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_futures_expiry ON futures_raw(expiry)')
        
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_stocks_trade_date ON stocks_raw(trade_date)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_stocks_symbol ON stocks_raw(symbol)')
        
        conn.commit()
        conn.close()
    
    def _get_connection(self):
        """å–å¾—è³‡æ–™åº«é€£ç·š"""
        return sqlite3.connect(self.db_path)
    
    def _restore_normal_settings(self):
        """æ¢å¾©æ­£å¸¸è³‡æ–™åº«è¨­å®š"""
        try:
            conn = self._get_connection()
            cursor = conn.cursor()
            cursor.execute("PRAGMA synchronous = NORMAL")
            cursor.execute("PRAGMA journal_mode = WAL")
            conn.close()
        except Exception as e:
            logging.error(f"æ¢å¾©æ­£å¸¸è¨­å®šå¤±æ•—: {e}")
    
    # === å¿«é€Ÿæ‰¹æ¬¡æ’å…¥æ–¹æ³• ===
    def batch_insert_options_fast(self, options_list: List[Dict[str, Any]]) -> int:
        """å¿«é€Ÿæ‰¹æ¬¡æ’å…¥é¸æ“‡æ¬Šè³‡æ–™ - é‡å°å¤§é‡è³‡æ–™å„ªåŒ–"""
        if not options_list:
            return 0
        
        try:
            conn = self._get_connection()
            cursor = conn.cursor()
            
            # é–‹å§‹äº‹å‹™ä¸¦å„ªåŒ–è¨­å®š
            cursor.execute("BEGIN TRANSACTION")
            cursor.execute("PRAGMA synchronous = OFF")
            cursor.execute("PRAGMA journal_mode = MEMORY")
            cursor.execute("PRAGMA cache_size = 10000")
            cursor.execute("PRAGMA temp_store = MEMORY")
            
            # æº–å‚™æ‰¹æ¬¡æ’å…¥è³‡æ–™
            data_tuples = []
            for opt in options_list:
                data_tuples.append((
                    opt['product'],
                    opt['trade_date'],
                    opt['expiry'],
                    opt['strike'],
                    opt['cp'],
                    opt.get('volume', 0),
                    opt.get('oi'),
                    opt.get('raw_oi_text'),
                    opt.get('session', 'regular'),
                    opt.get('load_file')
                ))
            
            # æ‰¹æ¬¡æ’å…¥
            cursor.executemany('''
                INSERT OR IGNORE INTO options_raw 
                (product, trade_date, expiry, strike, cp, volume, oi, raw_oi_text, session, load_file)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', data_tuples)
            
            conn.commit()
            affected_rows = cursor.rowcount
            conn.close()
            
            # æ¢å¾©æ­£å¸¸è¨­å®š
            self._restore_normal_settings()
            return affected_rows
            
        except Exception as e:
            logging.error(f"æ‰¹æ¬¡æ’å…¥é¸æ“‡æ¬Šè³‡æ–™å¤±æ•—: {e}")
            self._restore_normal_settings()
            return 0
    
    def batch_insert_futures_fast(self, futures_list: List[Dict[str, Any]]) -> int:
        """å¿«é€Ÿæ‰¹æ¬¡æ’å…¥æœŸè²¨è³‡æ–™"""
        if not futures_list:
            return 0
        
        try:
            conn = self._get_connection()
            cursor = conn.cursor()
            
            cursor.execute("BEGIN TRANSACTION")
            cursor.execute("PRAGMA synchronous = OFF")
            cursor.execute("PRAGMA journal_mode = MEMORY")
            
            data_tuples = []
            for future in futures_list:
                data_tuples.append((
                    future['product'],
                    future['trade_date'],
                    future['expiry'],
                    future.get('open'),
                    future.get('high'),
                    future.get('low'),
                    future.get('close'),
                    future.get('volume', 0),
                    future.get('oi', 0),
                    future.get('settlement'),
                    future.get('session', 'regular'),
                    future.get('load_file')
                ))
            
            cursor.executemany('''
                INSERT OR IGNORE INTO futures_raw 
                (product, trade_date, expiry, open, high, low, close, volume, oi, settlement, session, load_file)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', data_tuples)
            
            conn.commit()
            affected_rows = cursor.rowcount
            conn.close()
            
            self._restore_normal_settings()
            return affected_rows
            
        except Exception as e:
            logging.error(f"æ‰¹æ¬¡æ’å…¥æœŸè²¨è³‡æ–™å¤±æ•—: {e}")
            self._restore_normal_settings()
            return 0
    
    def batch_insert_stocks_fast(self, stocks_list: List[Dict[str, Any]]) -> int:
        """å¿«é€Ÿæ‰¹æ¬¡æ’å…¥è‚¡ç¥¨è³‡æ–™"""
        if not stocks_list:
            return 0
        
        try:
            conn = self._get_connection()
            cursor = conn.cursor()
            
            cursor.execute("BEGIN TRANSACTION")
            cursor.execute("PRAGMA synchronous = OFF")
            cursor.execute("PRAGMA journal_mode = MEMORY")
            
            data_tuples = []
            for stock in stocks_list:
                data_tuples.append((
                    stock['symbol'],
                    stock.get('chinese_name'),
                    stock['trade_date'],
                    stock.get('open'),
                    stock.get('high'),
                    stock.get('low'),
                    stock.get('close'),
                    stock.get('volume', 0),
                    stock.get('value', 0),
                    stock.get('load_file')
                ))
            
            cursor.executemany('''
                INSERT OR IGNORE INTO stocks_raw 
                (symbol, chinese_name, trade_date, open, high, low, close, volume, value, load_file)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', data_tuples)
            
            conn.commit()
            affected_rows = cursor.rowcount
            conn.close()
            
            self._restore_normal_settings()
            return affected_rows
            
        except Exception as e:
            logging.error(f"æ‰¹æ¬¡æ’å…¥è‚¡ç¥¨è³‡æ–™å¤±æ•—: {e}")
            self._restore_normal_settings()
            return 0
    
    # === æŸ¥è©¢æ“ä½œ ===
    def query_options(self, product=None, trade_date=None, expiry=None):
        """æŸ¥è©¢é¸æ“‡æ¬Šè³‡æ–™"""
        conn = self._get_connection()
        query = "SELECT * FROM options_raw WHERE 1=1"
        params = []
        
        if product:
            query += " AND product = ?"
            params.append(product)
        if trade_date:
            query += " AND trade_date = ?"
            params.append(trade_date)
        if expiry:
            query += " AND expiry = ?"
            params.append(expiry)
        
        query += " ORDER BY trade_date DESC, strike, cp"
        df = pd.read_sql_query(query, conn, params=params)
        conn.close()
        return df
    
    def query_futures(self, product=None, trade_date=None):
        """æŸ¥è©¢æœŸè²¨è³‡æ–™"""
        conn = self._get_connection()
        query = "SELECT * FROM futures_raw WHERE 1=1"
        params = []
        
        if product:
            query += " AND product = ?"
            params.append(product)
        if trade_date:
            query += " AND trade_date = ?"
            params.append(trade_date)
        
        query += " ORDER BY trade_date DESC"
        df = pd.read_sql_query(query, conn, params=params)
        conn.close()
        return df
    
    def query_stocks(self, symbol=None, trade_date=None):
        """æŸ¥è©¢è‚¡ç¥¨è³‡æ–™"""
        conn = self._get_connection()
        query = "SELECT * FROM stocks_raw WHERE 1=1"
        params = []
        
        if symbol:
            query += " AND symbol = ?"
            params.append(symbol)
        if trade_date:
            query += " AND trade_date = ?"
            params.append(trade_date)
        
        query += " ORDER BY trade_date DESC"
        df = pd.read_sql_query(query, conn, params=params)
        conn.close()
        return df
    
    # === è³‡æ–™åº«ç®¡ç† ===
    def get_database_info(self):
        """å–å¾—è³‡æ–™åº«è³‡è¨Š"""
        conn = self._get_connection()
        cursor = conn.cursor()
        
        info = {}
        
        # å–å¾—å„è¡¨æ ¼è³‡æ–™ç­†æ•¸
        cursor.execute("SELECT COUNT(*) FROM options_raw")
        info['options_count'] = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM futures_raw")
        info['futures_count'] = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM stocks_raw")
        info['stocks_count'] = cursor.fetchone()[0]
        
        # å–å¾—è³‡æ–™æ—¥æœŸç¯„åœ
        cursor.execute("SELECT MIN(trade_date), MAX(trade_date) FROM options_raw")
        info['options_date_range'] = cursor.fetchone()
        
        cursor.execute("SELECT MIN(trade_date), MAX(trade_date) FROM futures_raw")
        info['futures_date_range'] = cursor.fetchone()
        
        cursor.execute("SELECT MIN(trade_date), MAX(trade_date) FROM stocks_raw")
        info['stocks_date_range'] = cursor.fetchone()
        
        conn.close()
        return info

class EnhancedTXODataScraper:
    def __init__(self, root):
        self.root = root
        self.root.title("å°è‚¡å¸‚å ´è³‡æ–™æ“·å–èˆ‡åˆ†æå·¥å…· + é‡‘èè³‡æ–™åº«")
        self.root.geometry("1400x900")
        self.root.configure(bg='black')
        
        # è¨­å®šå­—é«”
        self.font_style = ("Microsoft JhengHei", 11)
        self.title_font = ("Microsoft JhengHei", 14, "bold")
        self.mono_font = ("Consolas", 10)
        
        # å„²å­˜çµæ§‹åŒ–è³‡æ–™
        self.structured_data = None
        self.current_url = ""
        self.analysis_results = None
        
        # åˆå§‹åŒ–é‡‘èè³‡æ–™åº«
        self.database = FinancialDatabase()
        
        # å°è‚¡è³‡æ–™ç¶²å€æ¸…å–®
        self.taiwan_market_urls = self.load_market_urls()
        
        self.setup_gui()
        
    def load_market_urls(self):
        """è¼‰å…¥å°è‚¡å¸‚å ´è³‡æ–™ç¶²å€æ¸…å–®ï¼ˆåˆ†é¡æ•´ç†ï¼‰"""
        urls = {
            # === é«˜é »è³‡æ–™ (HF) ===
            "HF-é¸æ“‡æ¬Šæ—¥å ±è¡¨": "https://www.taifex.com.tw/cht/3/optDailyMarketReport",
            "HF-æœŸè²¨æ—¥å ±è¡¨": "https://www.taifex.com.tw/cht/3/dlFutDailyMarketView",
            
            # === é¸æ“‡æ¬Šè³‡æ–™ ===
            "é¸æ“‡æ¬Š-æœªå¹³å€‰é¤˜é¡": "https://www.taifex.com.tw/cht/3/optContractsDate",
            "é¸æ“‡æ¬Š-æ—¥å ±è¡¨": "https://www.taifex.com.tw/cht/3/optDailyMarketReport", 
            "é¸æ“‡æ¬Š-æ­·å²è³‡æ–™": "https://www.taifex.com.tw/cht/3/optPrevious30DaysSalesData",
            "é¸æ“‡æ¬Š-è²·è³£æ¬Šåˆ†è¨ˆ": "https://www.taifex.com.tw/cht/3/callsAndPutsDate",
            
            # === æœŸè²¨è³‡æ–™ ===
            "æœŸè²¨-æ—¥å ±è¡¨": "https://www.taifex.com.tw/cht/3/futDailyMarketReport",
            "æœŸè²¨-æ­·å²è³‡æ–™": "https://www.taifex.com.tw/cht/3/futPrevious30DaysSalesData",
            "æœŸè²¨-æœªå¹³å€‰é¤˜é¡": "https://www.taifex.com.tw/cht/3/futContractsDate",
            
            # === ä¸‰å¤§æ³•äºº ===
            "æ³•äºº-æœŸè²¨æœªå¹³å€‰": "https://www.taifex.com.tw/cht/3/futContractsDate",
            "æ³•äºº-é¸æ“‡æ¬Šæœªå¹³å€‰": "https://www.taifex.com.tw/cht/3/optContractsDate",
            "æ³•äºº-å¤–è³‡æœªå¹³å€‰": "https://www.taifex.com.tw/cht/3/internationalTreats",
            
            # === ç›¤å¾Œè³‡æ–™ä¸‹è¼‰ ===
            "ç›¤å¾Œ-æœŸè²¨è³‡æ–™": "https://www.taifex.com.tw/cht/3/dlFutDataDown",
            "ç›¤å¾Œ-é¸æ“‡æ¬Šè³‡æ–™": "https://www.taifex.com.tw/cht/3/dlOptDataDown",
            "ç›¤å¾Œ-æ¯ç­†æˆäº¤": "https://www.taifex.com.tw/cht/3/dlFutTxfDown",
            
            # === æŒ‡æ•¸èˆ‡æ³¢å‹•ç‡ ===
            "æŒ‡æ•¸-æ³¢å‹•ç‡æŒ‡æ•¸": "https://www.taifex.com.tw/cht/7/vixChart",
            "æŒ‡æ•¸-ç›¤å¾Œè¡Œæƒ…": "https://www.taifex.com.tw/cht/3/futMarketReport",
            
            # === è­‰äº¤æ‰€è³‡æ–™ ===
            "è­‰äº¤æ‰€-å€‹è‚¡æ—¥æˆäº¤": "https://www.twse.com.tw/zh/page/trading/exchange/STOCK_DAY.html",
            "è­‰äº¤æ‰€-ä¸‰å¤§æ³•äºº": "https://www.twse.com.tw/zh/page/trading/fund/BFI82U.html",
            "è­‰äº¤æ‰€-èè³‡èåˆ¸": "https://www.twse.com.tw/zh/page/trading/exchange/MI_MARGN.html",
            "è­‰äº¤æ‰€-è‚¡åƒ¹æŒ‡æ•¸": "https://www.twse.com.tw/zh/page/trading/indices/MI_5MINS_HIST.html",
            "è­‰äº¤æ‰€-å€‹è‚¡é€±è½‰ç‡": "https://www.twse.com.tw/zh/page/trading/exchange/STOCK_DAY_AVG.html",
            
            # === æ«ƒè²·ä¸­å¿ƒ ===
            "æ«ƒè²·-å€‹è‚¡æ—¥æˆäº¤": "https://www.tpex.org.tw/web/stock/aftertrading/daily_trading_info/st43_result.php",
            "æ«ƒè²·-ä¸‰å¤§æ³•äºº": "https://www.tpex.org.tw/web/stock/3insti/3insti_summary/3itrdsum_result.php",
            
            # === å…¬é–‹è³‡è¨Šè§€æ¸¬ç«™ ===
            "å…¬é–‹è³‡è¨Š-è²¡å‹™å ±è¡¨": "https://mops.twse.com.tw/mops/web/t51sb01",
            
            # === Yahoo Finance ===
            "YF-å°è‚¡å¤§ç›¤": "https://finance.yahoo.com/quote/%5ETWII/history/",
            "YF-å°ç©é›»": "https://finance.yahoo.com/quote/2330.TW/history/",
            "YF-è¯ç™¼ç§‘": "https://finance.yahoo.com/quote/2454.TW/history/",
            "YF-é´»æµ·": "https://finance.yahoo.com/quote/2317.TW/history/"
        }
        return urls

    def setup_gui(self):
        """è¨­å®šåœ–å½¢åŒ–ä½¿ç”¨è€…ä»‹é¢"""
        # ä¸»æ¡†æ¶
        main_frame = tk.Frame(self.root, bg='black')
        main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)
        
        # æ¨™é¡Œ
        title_label = tk.Label(main_frame, text="å°è‚¡å¸‚å ´è³‡æ–™æ“·å–èˆ‡åˆ†æå·¥å…· + é‡‘èè³‡æ–™åº«", 
                              font=self.title_font, fg='white', bg='black')
        title_label.pack(pady=10)
        
        # URLé¸æ“‡æ¡†æ¶
        url_select_frame = tk.Frame(main_frame, bg='black')
        url_select_frame.pack(fill=tk.X, pady=5)
        
        tk.Label(url_select_frame, text="é¸æ“‡è³‡æ–™ä¾†æº:", font=self.font_style, 
                fg='white', bg='black').pack(side=tk.LEFT)
        
        # å»ºç«‹ä¸‹æ‹‰é¸å–®
        self.url_var = tk.StringVar()
        self.url_combo = ttk.Combobox(url_select_frame, textvariable=self.url_var, 
                                     font=self.font_style, width=80, state="readonly")
        self.url_combo['values'] = list(self.taiwan_market_urls.keys())
        self.url_combo.pack(side=tk.LEFT, padx=5, fill=tk.X, expand=True)
        self.url_combo.bind('<<ComboboxSelected>>', self.on_url_selected)
        
        # è‡ªè¨‚URLæ¡†æ¶
        custom_url_frame = tk.Frame(main_frame, bg='black')
        custom_url_frame.pack(fill=tk.X, pady=5)
        
        tk.Label(custom_url_frame, text="æˆ–è¼¸å…¥è‡ªè¨‚ç¶²å€:", font=self.font_style, 
                fg='white', bg='black').pack(side=tk.LEFT)
        
        self.custom_url_var = tk.StringVar()
        self.custom_url_entry = tk.Entry(custom_url_frame, textvariable=self.custom_url_var, 
                                        font=self.font_style, width=80, bg='white', fg='black')
        self.custom_url_entry.pack(side=tk.LEFT, padx=5, fill=tk.X, expand=True)
        self.custom_url_entry.bind('<Return>', self.on_custom_url_entered)
        
        # åŠŸèƒ½æŒ‰éˆ•æ¡†æ¶
        button_frame = tk.Frame(main_frame, bg='black')
        button_frame.pack(fill=tk.X, pady=10)
        
        # ç¬¬ä¸€æ’æŒ‰éˆ•ï¼šç¶²é åˆ†æåŠŸèƒ½
        buttons_row1 = [
            ("ğŸ” åˆ†æä¸‹è¼‰é€£çµ", self.analyze_download_links),
            ("ğŸ“Š æ“·å–ä¸¦è§£æ", self.fetch_and_parse),
            ("ğŸ“ˆ é¡¯ç¤ºçµæ§‹åŒ–è³‡æ–™", self.show_structured_data),
            ("ğŸ’¾ åŒ¯å‡ºJSON", self.export_structured_json),
            ("ğŸ“ åŒ¯å‡ºCSV", self.export_structured_csv),
        ]
        
        for text, command in buttons_row1:
            tk.Button(button_frame, text=text, font=self.font_style, 
                     command=command, bg='white', fg='black').pack(side=tk.LEFT, padx=2)
        
        # ç¬¬äºŒæ’æŒ‰éˆ•ï¼šè³‡æ–™åº«åŠŸèƒ½
        button_frame2 = tk.Frame(main_frame, bg='black')
        button_frame2.pack(fill=tk.X, pady=5)
        
        buttons_row2 = [
            ("ğŸ—ƒï¸ è³‡æ–™åº«è³‡è¨Š", self.show_database_info),
            ("ğŸ“¥ åŒ¯å…¥CSVåˆ°è³‡æ–™åº«", self.import_csv_to_database),
            ("ğŸ“¤ åŒ¯å‡ºè³‡æ–™åº«æŸ¥è©¢", self.export_database_query),
            ("ğŸ” æŸ¥è©¢é¸æ“‡æ¬Š", self.query_options),
            ("ğŸ” æŸ¥è©¢æœŸè²¨", self.query_futures),
            ("ğŸ” æŸ¥è©¢è‚¡ç¥¨", self.query_stocks),
        ]
        
        for text, command in buttons_row2:
            tk.Button(button_frame2, text=text, font=self.font_style, 
                     command=command, bg='lightblue', fg='black').pack(side=tk.LEFT, padx=2)
        
        # çµæœé¡¯ç¤ºå€åŸŸ
        result_frame = tk.Frame(main_frame, bg='black')
        result_frame.pack(fill=tk.BOTH, expand=True, pady=10)
        
        # å»ºç«‹åˆ†é 
        self.notebook = ttk.Notebook(result_frame)
        self.notebook.pack(fill=tk.BOTH, expand=True)
        
        # åˆ†æçµæœåˆ†é 
        self.analysis_text = scrolledtext.ScrolledText(
            self.notebook, font=self.mono_font, bg='black', fg='white'
        )
        self.notebook.add(self.analysis_text, text="ğŸ“Š ä¸‹è¼‰é€£çµåˆ†æ")
        
        # åŸå§‹è³‡æ–™åˆ†é 
        self.raw_text = scrolledtext.ScrolledText(
            self.notebook, font=self.mono_font, bg='black', fg='white'
        )
        self.notebook.add(self.raw_text, text="åŸå§‹è³‡æ–™")
        
        # çµæ§‹åŒ–è³‡æ–™åˆ†é 
        self.structured_text = scrolledtext.ScrolledText(
            self.notebook, font=self.mono_font, bg='black', fg='white'
        )
        self.notebook.add(self.structured_text, text="çµæ§‹åŒ–è³‡æ–™")
        
        # è³‡æ–™åº«åˆ†é 
        self.database_text = scrolledtext.ScrolledText(
            self.notebook, font=self.mono_font, bg='black', fg='white'
        )
        self.notebook.add(self.database_text, text="ğŸ—ƒï¸ è³‡æ–™åº«")
        
        # ç‹€æ…‹æ¬„
        self.status_var = tk.StringVar(value="å°±ç·’ - è«‹é¸æ“‡è³‡æ–™ä¾†æº")
        status_bar = tk.Label(main_frame, textvariable=self.status_var, 
                             font=self.font_style, fg='white', bg='black', 
                             anchor=tk.W)
        status_bar.pack(fill=tk.X)
        
        # è¨­å®šé è¨­é¸é …
        self.url_combo.set("HF-é¸æ“‡æ¬Šæ—¥å ±è¡¨")
        self.on_url_selected()

    def on_url_selected(self, event=None):
        """ç•¶é¸æ“‡é è¨­ç¶²å€æ™‚"""
        selected = self.url_var.get()
        if selected in self.taiwan_market_urls:
            url = self.taiwan_market_urls[selected]
            self.custom_url_var.set(url)
            self.update_status(f"å·²é¸æ“‡: {selected}")

    def on_custom_url_entered(self, event=None):
        """ç•¶è¼¸å…¥è‡ªè¨‚ç¶²å€æ™‚"""
        custom_url = self.custom_url_var.get().strip()
        if custom_url:
            self.url_var.set("")  # æ¸…ç©ºä¸‹æ‹‰é¸å–®é¸æ“‡
            self.update_status(f"å·²è¼¸å…¥è‡ªè¨‚ç¶²å€: {custom_url}")

    def get_current_url(self):
        """å–å¾—ç•¶å‰ç¶²å€"""
        custom_url = self.custom_url_var.get().strip()
        if custom_url:
            return custom_url
        selected = self.url_var.get()
        return self.taiwan_market_urls.get(selected, "")

    def update_status(self, message):
        """æ›´æ–°ç‹€æ…‹æ¬„"""
        self.status_var.set(message)
        self.root.update()

    def analyze_download_links(self):
        """åˆ†æç¶²é ä¸­çš„ä¸‹è¼‰é€£çµ"""
        url = self.get_current_url()
        if not url:
            messagebox.showwarning("è­¦å‘Š", "è«‹é¸æ“‡æˆ–è¼¸å…¥ç¶²å€")
            return
            
        try:
            self.update_status("æ­£åœ¨åˆ†æä¸‹è¼‰é€£çµ...")
            self.current_url = url
            
            # åœ¨èƒŒæ™¯åŸ·è¡Œåˆ†æ
            thread = threading.Thread(target=self._analyze_in_thread, args=(url,))
            thread.daemon = True
            thread.start()
            
        except Exception as e:
            error_msg = f"åˆ†æå¤±æ•—: {e}"
            logging.error(error_msg)
            messagebox.showerror("éŒ¯èª¤", error_msg)
            self.update_status("åˆ†æå¤±æ•—")

    def _analyze_in_thread(self, url):
        """åœ¨èƒŒæ™¯åŸ·è¡Œåˆ†æ"""
        try:
            results = self.monitor_requests(url)
            self.root.after(0, self._display_analysis_results, results, url)
        except Exception as e:
            self.root.after(0, self._analysis_failed, str(e))

    def _display_analysis_results(self, results, url):
        """é¡¯ç¤ºåˆ†æçµæœ"""
        self.update_status("åˆ†æå®Œæˆ")
        
        if not results:
            messagebox.showerror("éŒ¯èª¤", "åˆ†æå¤±æ•—")
            return
        
        self.analysis_results = results
        
        # é¡¯ç¤ºåˆ†æçµæœ
        self.analysis_text.delete(1.0, tk.END)
        
        # æ‰¾å‡ºå¯èƒ½åŒ…å«è¡¨æ ¼çš„é€£çµï¼ˆå„ªå…ˆé¡¯ç¤ºï¼‰
        table_potential_links = []
        table_keywords = ['report', 'data', 'market', 'daily', 'æ­·å²', 'å ±è¡¨', 'è³‡æ–™', 'csv', 'excel', 'download', 'export', 'ä¸‹è¼‰', 'åŒ¯å‡º']
        
        overview_content = f"""ğŸŒ ç¶²é åˆ†æçµæœ: {url}
åˆ†ææ™‚é–“: {results['analysis_time']}
{'='*60}

ğŸ“Š çµ±è¨ˆè³‡è¨Š:
â€¢ æ‰¾åˆ°ä¸‹è¼‰é€£çµ: {len(results['download_links'])} å€‹
â€¢ æ‰¾åˆ°è¡¨å–®: {len(results['forms'])} å€‹
â€¢ æ‰¾åˆ°JavaScriptä¸‹è¼‰åŠŸèƒ½: {len(results['js_downloads'])} å€‹

ğŸ’¡ åˆ†ææ‘˜è¦:
"""
        
        if results['download_links']:
            overview_content += "âœ… ç™¼ç¾ç›´æ¥ä¸‹è¼‰é€£çµ\n"
            
            # æ‰¾å‡ºå¯èƒ½åŒ…å«è¡¨æ ¼çš„é€£çµ
            for link in results['download_links']:
                for keyword in table_keywords:
                    if keyword in link['url'].lower() or keyword in link['text'].lower():
                        table_potential_links.append(link)
                        break
        else:
            overview_content += "âŒ æœªç™¼ç¾ç›´æ¥ä¸‹è¼‰é€£çµ\n"
            
        if any(form['likely_download'] for form in results['forms']):
            overview_content += "âœ… ç™¼ç¾å¯èƒ½çš„ä¸‹è¼‰è¡¨å–®\n"
        else:
            overview_content += "âŒ æœªç™¼ç¾ä¸‹è¼‰è¡¨å–®\n"
            
        if results['js_downloads']:
            overview_content += "âœ… ç™¼ç¾JavaScriptä¸‹è¼‰åŠŸèƒ½\n"
        else:
            overview_content += "âŒ æœªç™¼ç¾JavaScriptä¸‹è¼‰åŠŸèƒ½\n"
        
        # å„ªå…ˆé¡¯ç¤ºå¯èƒ½åŒ…å«è¡¨æ ¼çš„é€£çµ
        if table_potential_links:
            overview_content += f"\nğŸ” ç™¼ç¾ {len(table_potential_links)} å€‹å¯èƒ½åŒ…å«è¡¨æ ¼è³‡æ–™çš„é€£çµ (å„ªå…ˆè™•ç†):\n"
            overview_content += "="*60 + "\n"
            for i, link in enumerate(table_potential_links, 1):
                overview_content += f"{i}. {link['text']}\n"
                overview_content += f"   ğŸ“ URL: {link['url']}\n"
                overview_content += f"   ğŸ·ï¸ é¡å‹: {link['type']}\n"
                overview_content += "-" * 40 + "\n"
        
        self.analysis_text.insert(tk.END, overview_content)
        
        # é¡¯ç¤ºæ‰€æœ‰ä¸‹è¼‰é€£çµ
        if results['download_links']:
            links_content = "\nğŸ”— æ‰€æœ‰ä¸‹è¼‰é€£çµ:\n" + "="*50 + "\n\n"
            for i, link in enumerate(results['download_links'], 1):
                links_content += f"{i}. {link['text']}\n"
                links_content += f"   ğŸ“ URL: {link['url']}\n"
                links_content += f"   ğŸ·ï¸ é¡å‹: {link['type']}\n"
                links_content += f"   ğŸ” é—œéµå­—: {link['keyword']}\n"
                links_content += "-" * 40 + "\n"
            
            self.analysis_text.insert(tk.END, links_content)
        
        messagebox.showinfo("å®Œæˆ", "ç¶²é åˆ†æå®Œæˆï¼è«‹æŸ¥çœ‹ã€ä¸‹è¼‰é€£çµåˆ†æã€åˆ†é ")

    def _analysis_failed(self, error_msg):
        """åˆ†æå¤±æ•—è™•ç†"""
        self.update_status("åˆ†æå¤±æ•—")
        messagebox.showerror("éŒ¯èª¤", f"åˆ†æå¤±æ•—:\n{error_msg}")

    def monitor_requests(self, target_url):
        """ç›£æ§ç¶²é è«‹æ±‚"""
        try:
            session = requests.Session()
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
            }
            
            response = session.get(target_url, headers=headers)
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            download_links = self._find_download_links(soup, target_url)
            forms = self._analyze_forms(soup, target_url)
            js_downloads = self._find_js_downloads(soup)
            
            results = {
                'analysis_time': datetime.now().isoformat(),
                'target_url': target_url,
                'download_links': download_links,
                'forms': forms,
                'js_downloads': js_downloads
            }
            
            return results
            
        except Exception as e:
            print(f"åˆ†æå¤±æ•—: {e}")
            return None

    def _find_download_links(self, soup, base_url):
        download_keywords = ['download', 'csv', 'excel', 'data', 'export', 'ä¸‹è¼‰', 'åŒ¯å‡º', 'report', 'æ­·å²', 'xls', 'xlsx']
        download_links = []
        
        for link in soup.find_all('a', href=True):
            href = link['href'].lower()
            link_text = link.get_text().lower()
            
            for keyword in download_keywords:
                if keyword in href or keyword in link_text:
                    full_url = urljoin(base_url, link['href'])
                    download_links.append({
                        'url': full_url,
                        'text': link.get_text().strip(),
                        'type': 'direct_link',
                        'keyword': keyword
                    })
                    break
        
        return download_links

    def _analyze_forms(self, soup, base_url):
        forms_info = []
        
        for form in soup.find_all('form'):
            form_info = {
                'action': form.get('action', ''),
                'method': form.get('method', 'get').upper(),
                'inputs': [],
                'full_url': '',
                'likely_download': False
            }
            
            if form_info['action']:
                form_info['full_url'] = urljoin(base_url, form_info['action'])
            else:
                form_info['full_url'] = base_url
            
            for input_tag in form.find_all(['input', 'select', 'textarea']):
                input_info = {
                    'type': input_tag.name,
                    'name': input_tag.get('name', ''),
                    'value': input_tag.get('value', ''),
                    'input_type': input_tag.get('type', '')
                }
                form_info['inputs'].append(input_info)
            
            # ç°¡å–®åˆ¤æ–·æ˜¯å¦ç‚ºä¸‹è¼‰è¡¨å–®
            action_lower = form_info['action'].lower()
            download_indicators = ['download', 'export', 'csv', 'excel', 'data', 'ä¸‹è¼‰', 'åŒ¯å‡º']
            for indicator in download_indicators:
                if indicator in action_lower:
                    form_info['likely_download'] = True
                    break
            
            forms_info.append(form_info)
        
        return forms_info

    def _find_js_downloads(self, soup):
        js_downloads = []
        download_keywords = ['download', 'csv', 'export', 'DataDown', 'getData', 'ä¸‹è¼‰', 'åŒ¯å‡º', 'excel', 'xls']
        
        for script in soup.find_all('script'):
            if script.string:
                script_content = script.string.lower()
                for keyword in download_keywords:
                    if keyword in script_content:
                        js_downloads.append({
                            'type': 'javascript',
                            'keyword': keyword,
                            'snippet': script.string[:200] + '...' if len(script.string) > 200 else script.string
                        })
                        break
        
        return js_downloads

    def fetch_and_parse(self):
        """æ“·å–ç¶²é ä¸¦è§£æç‚ºçµæ§‹åŒ–è³‡æ–™"""
        url = self.get_current_url()
        if not url:
            messagebox.showwarning("è­¦å‘Š", "è«‹é¸æ“‡æˆ–è¼¸å…¥ç¶²å€")
            return
            
        try:
            self.update_status("æ­£åœ¨é€£æ¥ç¶²ç«™...")
            self.current_url = url
            
            # è¨­å®šè«‹æ±‚é ­
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            }
            
            # ç™¼é€è«‹æ±‚
            response = requests.get(url, headers=headers, timeout=30)
            response.encoding = 'utf-8'
            response.raise_for_status()
            
            # é¡¯ç¤ºåŸå§‹è³‡æ–™
            self.raw_text.delete(1.0, tk.END)
            self.raw_text.insert(tk.END, f"URL: {url}\n")
            self.raw_text.insert(tk.END, f"ç‹€æ…‹ç¢¼: {response.status_code}\n")
            self.raw_text.insert(tk.END, f"è³‡æ–™é•·åº¦: {len(response.text)} å­—å…ƒ\n\n")
            self.raw_text.insert(tk.END, response.text[:5000] + "\n...")  # é¡¯ç¤ºå‰5000å­—å…ƒ
            
            # è§£æç‚ºçµæ§‹åŒ–è³‡æ–™
            self.structured_data = self.parse_to_structured_data(response.text, url)
            
            self.update_status(f"æˆåŠŸè§£æç‚ºçµæ§‹åŒ–è³‡æ–™ï¼Œæ‰¾åˆ° {len(self.structured_data['tables'])} å€‹è¡¨æ ¼")
            
            # é¡¯ç¤ºçµæ§‹åŒ–è³‡æ–™
            self.show_structured_data()
            
        except Exception as e:
            error_msg = f"æ“·å–è§£æå¤±æ•—: {e}"
            logging.error(error_msg)
            messagebox.showerror("éŒ¯èª¤", error_msg)
            self.update_status("æ“·å–å¤±æ•—")

    def parse_to_structured_data(self, html_content, url):
        """å°‡HTMLè§£æç‚ºçœŸæ­£çš„çµæ§‹åŒ–è³‡æ–™"""
        soup = BeautifulSoup(html_content, 'html.parser')
        tables = soup.find_all('table')
        
        structured_data = {
            'metadata': {
                'source_url': url,
                'scrape_time': datetime.now().isoformat(),
                'total_tables': len(tables),
                'data_format': 'structured_v1'
            },
            'tables': []
        }
        
        for i, table in enumerate(tables):
            table_data = self.parse_single_table(table, i + 1)
            if table_data:
                structured_data['tables'].append(table_data)
        
        return structured_data

    def parse_single_table(self, table, table_index):
        """è§£æå–®ä¸€è¡¨æ ¼ç‚ºçµæ§‹åŒ–è³‡æ–™"""
        try:
            # æå–è¡¨é ­
            headers = []
            header_rows = table.find_all(['th', 'td'])
            for th in header_rows:
                header_text = th.get_text(strip=True)
                if header_text:
                    headers.append(header_text)
            
            # æå–è³‡æ–™è¡Œ
            data_rows = []
            for tr in table.find_all('tr'):
                row_data = []
                for td in tr.find_all(['td', 'th']):
                    cell_text = td.get_text(strip=True)
                    row_data.append(cell_text)
                
                if row_data and len(row_data) > 1:  # éæ¿¾ç©ºè¡Œå’Œåªæœ‰ä¸€å€‹æ¬„ä½çš„è¡Œ
                    data_rows.append(row_data)
            
            if not data_rows:
                return None
            
            # å»ºç«‹çµæ§‹åŒ–è³‡æ–™
            table_structure = {
                'table_index': table_index,
                'columns': headers if headers else [f'Column_{j+1}' for j in range(len(data_rows[0]))],
                'row_count': len(data_rows),
                'data': []
            }
            
            # è½‰æ›ç‚ºå­—å…¸æ ¼å¼
            for row in data_rows:
                if len(row) == len(table_structure['columns']):
                    row_dict = {}
                    for j, value in enumerate(row):
                        column_name = table_structure['columns'][j] if j < len(table_structure['columns']) else f'Column_{j+1}'
                        row_dict[column_name] = value
                    table_structure['data'].append(row_dict)
                else:
                    # è™•ç†æ¬„ä½æ•¸é‡ä¸åŒ¹é…çš„æƒ…æ³
                    row_dict = {}
                    for j, value in enumerate(row):
                        column_name = f'Column_{j+1}'
                        row_dict[column_name] = value
                    table_structure['data'].append(row_dict)
            
            return table_structure
            
        except Exception as e:
            logging.error(f"è§£æè¡¨æ ¼ {table_index} å¤±æ•—: {e}")
            return None

    def show_structured_data(self):
        """é¡¯ç¤ºçµæ§‹åŒ–è³‡æ–™"""
        if not self.structured_data:
            messagebox.showwarning("è­¦å‘Š", "æ²’æœ‰å¯é¡¯ç¤ºçš„çµæ§‹åŒ–è³‡æ–™")
            return
            
        self.structured_text.delete(1.0, tk.END)
        
        # é¡¯ç¤ºå…ƒè³‡æ–™
        metadata = self.structured_data['metadata']
        self.structured_text.insert(tk.END, "=== å…ƒè³‡æ–™ ===\n")
        self.structured_text.insert(tk.END, f"ä¾†æºç¶²å€: {metadata['source_url']}\n")
        self.structured_text.insert(tk.END, f"æ“·å–æ™‚é–“: {metadata['scrape_time']}\n")
        self.structured_text.insert(tk.END, f"è¡¨æ ¼æ•¸é‡: {metadata['total_tables']}\n\n")
        
        # é¡¯ç¤ºæ¯å€‹è¡¨æ ¼çš„çµæ§‹åŒ–è³‡æ–™
        for table in self.structured_data['tables']:
            self.structured_text.insert(tk.END, f"=== è¡¨æ ¼ {table['table_index']} ===\n")
            self.structured_text.insert(tk.END, f"æ¬„ä½: {table['columns']}\n")
            self.structured_text.insert(tk.END, f"è³‡æ–™ç­†æ•¸: {table['row_count']}\n")
            self.structured_text.insert(tk.END, "å‰5ç­†è³‡æ–™:\n")
            
            # é¡¯ç¤ºå‰5ç­†è³‡æ–™
            for i, row in enumerate(table['data'][:5]):
                self.structured_text.insert(tk.END, f"ç¬¬{i+1}ç­†: {row}\n")
            
            self.structured_text.insert(tk.END, "\n")

    def export_structured_json(self):
        """åŒ¯å‡ºçµæ§‹åŒ–JSONè³‡æ–™"""
        if not self.structured_data:
            messagebox.showwarning("è­¦å‘Š", "æ²’æœ‰å¯åŒ¯å‡ºçš„çµæ§‹åŒ–è³‡æ–™")
            return
            
        try:
            filename = f"Data/structured_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.structured_data, f, ensure_ascii=False, indent=2)
            
            messagebox.showinfo("æˆåŠŸ", f"çµæ§‹åŒ–è³‡æ–™å·²åŒ¯å‡ºè‡³: {filename}")
            self.update_status(f"JSONåŒ¯å‡ºå®Œæˆ: {filename}")
            
        except Exception as e:
            error_msg = f"åŒ¯å‡ºJSONå¤±æ•—: {e}"
            messagebox.showerror("éŒ¯èª¤", error_msg)

    def export_structured_csv(self):
        """åŒ¯å‡ºçµæ§‹åŒ–CSVè³‡æ–™"""
        if not self.structured_data:
            messagebox.showwarning("è­¦å‘Š", "æ²’æœ‰å¯åŒ¯å‡ºçš„çµæ§‹åŒ–è³‡æ–™")
            return
            
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            for table in self.structured_data['tables']:
                filename = f"Data/table_{table['table_index']}_{timestamp}.csv"
                
                # è½‰æ›ç‚ºDataFrame
                df = pd.DataFrame(table['data'])
                df.to_csv(filename, index=False, encoding='utf-8-sig')
            
            messagebox.showinfo("æˆåŠŸ", f"CSVè³‡æ–™å·²åŒ¯å‡ºè‡³Dataè³‡æ–™å¤¾")
            self.update_status("CSVåŒ¯å‡ºå®Œæˆ")
            
        except Exception as e:
            error_msg = f"åŒ¯å‡ºCSVå¤±æ•—: {e}"
            messagebox.showerror("éŒ¯èª¤", error_msg)

    # === è³‡æ–™åº«ç›¸é—œæ–¹æ³•ï¼ˆæ”¹é€²ç‰ˆæœ¬ï¼‰===
    def show_database_info(self):
        """é¡¯ç¤ºè³‡æ–™åº«è³‡è¨Š"""
        try:
            info = self.database.get_database_info()
            
            self.database_text.delete(1.0, tk.END)
            content = "=== é‡‘èè³‡æ–™åº«è³‡è¨Š ===\n\n"
            content += f"ğŸ“Š é¸æ“‡æ¬Šè³‡æ–™ç­†æ•¸: {info['options_count']:,} ç­†\n"
            content += f"ğŸ“ˆ æœŸè²¨è³‡æ–™ç­†æ•¸: {info['futures_count']:,} ç­†\n"
            content += f"ğŸ¢ è‚¡ç¥¨è³‡æ–™ç­†æ•¸: {info['stocks_count']:,} ç­†\n\n"
            
            content += "ğŸ“… è³‡æ–™æ—¥æœŸç¯„åœ:\n"
            if info['options_date_range'][0]:
                content += f"   é¸æ“‡æ¬Š: {info['options_date_range'][0]} è‡³ {info['options_date_range'][1]}\n"
            if info['futures_date_range'][0]:
                content += f"   æœŸè²¨: {info['futures_date_range'][0]} è‡³ {info['futures_date_range'][1]}\n"
            if info['stocks_date_range'][0]:
                content += f"   è‚¡ç¥¨: {info['stocks_date_range'][0]} è‡³ {info['stocks_date_range'][1]}\n"
            
            content += f"\nğŸ’¾ è³‡æ–™åº«ä½ç½®: {self.database.db_path}"
            
            self.database_text.insert(tk.END, content)
            self.notebook.select(3)  # åˆ‡æ›åˆ°è³‡æ–™åº«åˆ†é 
            self.update_status("è³‡æ–™åº«è³‡è¨Šè¼‰å…¥å®Œæˆ")
            
        except Exception as e:
            messagebox.showerror("éŒ¯èª¤", f"å–å¾—è³‡æ–™åº«è³‡è¨Šå¤±æ•—: {e}")

    def import_csv_to_database(self):
        """å¿«é€ŸåŒ¯å…¥CSVæª”æ¡ˆåˆ°è³‡æ–™åº« - æ”¹é€²çš„æ™ºèƒ½åˆ†é¡é‚è¼¯"""
        try:
            file_path = filedialog.askopenfilename(
                title="é¸æ“‡CSVæª”æ¡ˆ",
                filetypes=[("CSVæª”æ¡ˆ", "*.csv"), ("æ‰€æœ‰æª”æ¡ˆ", "*.*")],
                initialdir="Data"
            )
            
            if not file_path:
                return
            
            file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB
            self.update_status(f"é–‹å§‹åŒ¯å…¥ {file_size:.1f}MB çš„CSVæª”æ¡ˆ...")
            
            # æ ¹æ“šæª”æ¡ˆå¤§å°æ±ºå®šchunkå¤§å°
            chunk_size = 50000 if file_size > 10 else 10000
            
            total_imported = 0
            start_time = datetime.now()
            
            # åˆ†æ‰¹è®€å–å¤§æª”æ¡ˆ
            for chunk_num, chunk_df in enumerate(pd.read_csv(file_path, chunksize=chunk_size)):
                self.update_status(f"è™•ç†ç¬¬ {chunk_num + 1} æ‰¹è³‡æ–™ ({len(chunk_df)} ç­†)...")
                
                filename = os.path.basename(file_path)
                import_count = self._process_data_chunk_fast(chunk_df, filename)
                total_imported += import_count
                
                # é¡¯ç¤ºé€²åº¦
                elapsed = (datetime.now() - start_time).total_seconds()
                rate = total_imported / elapsed if elapsed > 0 else 0
                self.update_status(f"å·²è™•ç†: {total_imported:,} ç­†, é€Ÿåº¦: {rate:.1f} ç­†/ç§’")
                
                # æ›´æ–°ä»‹é¢
                self.root.update()
            
            elapsed_time = (datetime.now() - start_time).total_seconds()
            messagebox.showinfo("å®Œæˆ", 
                              f"åŒ¯å…¥å®Œæˆï¼\n"
                              f"ç¸½å…±åŒ¯å…¥: {total_imported:,} ç­†è³‡æ–™\n"
                              f"èŠ±è²»æ™‚é–“: {elapsed_time:.1f} ç§’\n"
                              f"å¹³å‡é€Ÿåº¦: {total_imported/elapsed_time:.1f} ç­†/ç§’")
            
            self.update_status(f"å¿«é€ŸåŒ¯å…¥å®Œæˆ: {total_imported:,} ç­†è³‡æ–™")
            
        except Exception as e:
            messagebox.showerror("éŒ¯èª¤", f"åŒ¯å…¥CSVå¤±æ•—: {e}")
            self.update_status("åŒ¯å…¥å¤±æ•—")
    
    def _process_data_chunk_fast(self, chunk_df, filename):
        """æ”¹é€²çš„è‡ªå‹•åˆ†é¡é‚è¼¯ - åŒæ™‚æª¢æŸ¥ç¬¬ä¸€åˆ—å’Œç¬¬äºŒåˆ—ï¼Œä¿å­˜è‚¡ç¥¨ä»£è™Ÿè³‡è¨Š"""
        
        # ä¿å­˜åŸå§‹çš„ç¬¬0è¡Œå…§å®¹ï¼ˆå¯èƒ½åŒ…å«è‚¡ç¥¨ä»£è™Ÿï¼‰
        original_first_row = None
        if len(chunk_df) > 0:
            original_first_row = chunk_df.iloc[0].tolist()
        
        # 1. è¨ˆç®—åˆ†æ•¸ï¼ˆå…ˆæª¢æŸ¥ç¬¬ä¸€åˆ—ï¼Œåˆ†æ•¸ä¸è¶³æ‰æª¢æŸ¥ç¬¬äºŒåˆ—ï¼‰
        score_first_row = self._calculate_data_score(chunk_df.columns.tolist())
        score_second_row = 0
        if score_first_row < 3 and original_first_row:  # ç¬¬ä¸€åˆ—åˆ†æ•¸ä¸è¶³æ‰æª¢æŸ¥ç¬¬äºŒåˆ—
            score_second_row = self._calculate_data_score(original_first_row)
        
        symbol_info = None
        data_start_index = 0
        
        # 2. æ±ºç­–é‚è¼¯
        if score_first_row >= score_second_row:
            # ä½¿ç”¨ç¬¬ä¸€åˆ—ä½œç‚ºæ¬„ä½åç¨±
            column_names = set([str(col).lower() for col in chunk_df.columns.tolist()])
            data_start_index = 0
            self.update_status("ä½¿ç”¨ç¬¬ä¸€åˆ—ä½œç‚ºæ¬„ä½åç¨±")
        else:
            # ä½¿ç”¨ç¬¬äºŒåˆ—ä½œç‚ºæ¬„ä½åç¨±ï¼Œä½†å…ˆå¾è¢«è·³éçš„è¡Œæå–è‚¡ç¥¨ä»£è™Ÿ
            column_names = set([str(col).lower() for col in original_first_row])
            data_start_index = 1
            
            # å¾è¢«è·³éçš„ç¬¬0è¡Œå’Œç¬¬1è¡Œæå–è‚¡ç¥¨ä»£è™Ÿ
            symbol_info = self._extract_symbol_from_skipped_rows(chunk_df.columns.tolist(), original_first_row)
            
            # é‡æ–°å»ºç«‹DataFrameï¼ˆè·³éç¬¬0è¡Œï¼‰
            chunk_df = chunk_df.iloc[1:].reset_index(drop=True)
            self.update_status("ä½¿ç”¨ç¬¬äºŒåˆ—ä½œç‚ºæ¬„ä½åç¨±ï¼Œè·³éç¬¬ä¸€è¡Œæ–‡å­—èªªæ˜")
        
        # 3. è¨ˆç®—å„é¡è³‡æ–™åˆ†æ•¸
        option_indicators = {'cp', 'call/put', 'è²·è³£æ¬Š', 'strike', 'å±¥ç´„åƒ¹', 'expiry', 'åˆ°æœŸ'}
        future_indicators = {'settlement', 'çµç®—åƒ¹', 'oi', 'æœªå¹³å€‰', 'ç•™å€‰'}
        stock_indicators = {'open', 'high', 'low', 'close', 'volume', 'value', 'æˆäº¤é‡‘é¡', 'é–‹ç›¤', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›¤', 'æˆäº¤é‡'}
        
        option_score = len(column_names & option_indicators)
        future_score = len(column_names & future_indicators) 
        stock_score = len(column_names & stock_indicators)
        
        # 4. æ ¹æ“šåˆ†æ•¸æ±ºå®šè³‡æ–™é¡å‹
        if option_score >= 2:
            self.update_status(f"è­˜åˆ¥ç‚ºé¸æ“‡æ¬Šè³‡æ–™ (åˆ†æ•¸: {option_score})")
            return self._import_as_options(chunk_df, filename)
            
        elif future_score >= 2 and option_score == 0:
            self.update_status(f"è­˜åˆ¥ç‚ºæœŸè²¨è³‡æ–™ (åˆ†æ•¸: {future_score})")
            return self._import_as_futures(chunk_df, filename)
            
        elif stock_score >= 2 and option_score == 0 and future_score == 0:
            self.update_status(f"è­˜åˆ¥ç‚ºè‚¡ç¥¨è³‡æ–™ (åˆ†æ•¸: {stock_score})")
            # å¦‚æœæ˜¯è‚¡ç¥¨è³‡æ–™ï¼Œä½¿ç”¨æå–çš„è‚¡ç¥¨ä»£è™Ÿè³‡è¨Š
            if symbol_info:
                return self._import_as_stocks_with_symbol(chunk_df, filename, symbol_info)
            else:
                # å³ä½¿ä½¿ç”¨ç¬¬ä¸€åˆ—ä½œç‚ºæ¬„ä½åç¨±ï¼Œä¹Ÿå¯èƒ½åŒ…å«è‚¡ç¥¨ä»£è™Ÿ
                if data_start_index == 0:
                    symbol_info = self._extract_symbol_from_header(chunk_df.columns.tolist())
                    if symbol_info:
                        return self._import_as_stocks_with_symbol(chunk_df, filename, symbol_info)
                return self._import_as_stocks(chunk_df, filename)
            
        else:
            # ç„¡æ³•æ˜ç¢ºåˆ¤æ–·ï¼Œå˜—è©¦è‚¡ç¥¨ä»£è™Ÿè‡ªå‹•è¾¨è­˜
            self.update_status("ç¬¬ä¸€å±¤ç„¡æ³•åˆ¤æ–·ï¼Œé€²å…¥ç¬¬äºŒå±¤è‚¡ç¥¨ä»£è™Ÿè¾¨è­˜")
            symbol_info = self._auto_detect_stock_symbol(chunk_df)
            if symbol_info:
                self.update_status(f"ç¬¬äºŒå±¤è¾¨è­˜æˆåŠŸ: {symbol_info['symbol']} {symbol_info['chinese_name']}")
                return self._import_as_stocks_with_symbol(chunk_df, filename, symbol_info)
            else:
                # è®“ä½¿ç”¨è€…é¸æ“‡
                return self._ask_user_for_data_type(chunk_df, filename)

    def _extract_symbol_from_skipped_rows(self, first_row, second_row):
        """å¾è¢«è·³éçš„ç¬¬0è¡Œå’Œç¬¬1è¡Œä¸­æå–è‚¡ç¥¨ä»£è™Ÿ"""
        # åŒæ™‚æª¢æŸ¥ç¬¬0è¡Œå’Œç¬¬1è¡Œ
        symbol_info = self._extract_symbol_from_header(first_row)
        if not symbol_info:
            symbol_info = self._extract_symbol_from_header(second_row)
        
        return symbol_info

    def _extract_symbol_from_header(self, header_columns):
        """å¾è¡¨é ­è¾¨è­˜è‚¡ç¥¨ä»£è™Ÿ - ç²¾ç¢ºç‰ˆæœ¬"""
        import re
        
        for i, col in enumerate(header_columns):
            if isinstance(col, str):
                # ç²¾ç¢ºæ­£å‰‡ï¼šåœ¨æ•´å€‹å­—ä¸²ä¸­å°‹æ‰¾ã€Œç©ºæ ¼æˆ–é–‹é ­ + æ•¸å­—ä»£è™Ÿ + ç©ºæ ¼ + ä¸­æ–‡åç¨±ã€
                pattern = r'(?:\s|^)(\d{3,6}[A-Za-z]*)\s+([\u4e00-\u9fff]+)'
                # è§£é‡‹ï¼š
                # (?:\s|^) â†’ ç©ºæ ¼æˆ–å­—ä¸²é–‹é ­ï¼ˆéæ•ç²çµ„ï¼‰
                # (\d{3,6}[A-Za-z]*) â†’ 3-6ä½æ•¸å­—ï¼Œå¯èƒ½åŒ…å«è‹±æ–‡ï¼ˆè‚¡ç¥¨ä»£è™Ÿï¼‰
                # \s+ â†’ 1å€‹æˆ–å¤šå€‹ç©ºæ ¼
                # ([\u4e00-\u9fff]+) â†’ ä¸­æ–‡åç¨±
                
                match = re.search(pattern, col)
                
                if match:
                    symbol = match.group(1).strip()
                    chinese_name = match.group(2).strip()
                    
                    if self._is_valid_tw_stock_symbol(symbol):
                        return {
                            'symbol': symbol,
                            'chinese_name': chinese_name,
                            'found_in': 'header',
                            'column_index': i,
                            'original_text': col
                        }
        
        return None

    def _extract_symbol_from_data(self, data_row):
        """å¾è³‡æ–™è¡Œè¾¨è­˜è‚¡ç¥¨ä»£è™Ÿ"""
        import re
        
        for i, cell in enumerate(data_row):
            if isinstance(cell, str):
                # åŒæ¨£çš„ç²¾ç¢ºæ­£å‰‡
                pattern = r'(?:\s|^)(\d{3,6}[A-Za-z]*)\s+([\u4e00-\u9fff]+)'
                match = re.search(pattern, cell)
                
                if match:
                    symbol = match.group(1).strip()
                    chinese_name = match.group(2).strip()
                    
                    if self._is_valid_tw_stock_symbol(symbol):
                        return {
                            'symbol': symbol,
                            'chinese_name': chinese_name,
                            'found_in': 'data',
                            'column_index': i
                        }
        
        return None

    def _is_valid_tw_stock_symbol(self, symbol):
        """é©—è­‰æ˜¯å¦ç‚ºæœ‰æ•ˆçš„å°è‚¡è‚¡ç¥¨ä»£è™Ÿ"""
        import re
        
        # é•·åº¦æª¢æŸ¥
        if len(symbol) < 3 or len(symbol) > 6:
            return False
        
        # æ ¼å¼æª¢æŸ¥ï¼šå¿…é ˆä»¥æ•¸å­—é–‹é ­ï¼Œå¯èƒ½åŒ…å«è‹±æ–‡
        if not re.match(r'^\d+[A-Za-z]*$', symbol):
            return False
        
        # å¸¸è¦‹çš„å°è‚¡ä»£è™Ÿé•·åº¦
        if len(symbol) in [4, 5, 6]:
            return True
        
        return False

    def _calculate_data_score(self, columns):
        """è¨ˆç®—ä¸€çµ„æ–‡å­—ä½œç‚ºæ¬„ä½åç¨±çš„å¯ä¿¡åº¦åˆ†æ•¸"""
        if not columns:
            return 0
        
        score = 0
        column_texts = [str(col).lower() for col in columns]
        
        # å¸¸è¦‹çš„è‚¡ç¥¨è³‡æ–™æ¬„ä½é—œéµå­—
        stock_keywords = {
            'open', 'high', 'low', 'close', 'volume', 'value',
            'é–‹ç›¤', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›¤', 'æˆäº¤é‡', 'æˆäº¤é‡‘é¡',
            'æ—¥æœŸ', 'date', 'ä»£è™Ÿ', 'symbol', 'åç¨±', 'name'
        }
        
        # å¸¸è¦‹çš„é¸æ“‡æ¬Š/æœŸè²¨æ¬„ä½é—œéµå­—
        option_future_keywords = {
            'cp', 'call', 'put', 'strike', 'å±¥ç´„åƒ¹', 'expiry', 'åˆ°æœŸ',
            'settlement', 'çµç®—åƒ¹', 'oi', 'æœªå¹³å€‰', 'ç•™å€‰'
        }
        
        # å¸¸è¦‹çš„éæ¬„ä½åç¨±æ–‡å­—ï¼ˆæ–‡å­—èªªæ˜ï¼‰
        non_column_keywords = {
            'å ±å‘Š', 'å ±è¡¨', 'è³‡æ–™', 'çµ±è¨ˆ', 'æ˜ç´°', 'è¡¨', 'å¹´åº¦', 'æœˆä»½',
            'å…¬å¸', 'è‚¡ç¥¨', 'è­‰åˆ¸', 'äº¤æ˜“', 'å¸‚å ´', 'è¡Œæƒ…', 'æŠ•è³‡'
        }
        
        # è¨ˆç®—åˆ†æ•¸
        for text in column_texts:
            # å¦‚æœåŒ…å«è‚¡ç¥¨æ¬„ä½é—œéµå­—ï¼ŒåŠ åˆ†
            if any(keyword in text for keyword in stock_keywords):
                score += 2
            
            # å¦‚æœåŒ…å«é¸æ“‡æ¬Š/æœŸè²¨é—œéµå­—ï¼ŒåŠ åˆ†
            if any(keyword in text for keyword in option_future_keywords):
                score += 2
                
            # å¦‚æœçœ‹èµ·ä¾†åƒæ¬„ä½åç¨±ï¼ˆç°¡çŸ­ã€è‹±æ–‡æˆ–ç°¡çŸ­ä¸­æ–‡ï¼‰
            if len(text) <= 12 and not any(non_word in text for non_word in non_column_keywords):
                score += 1
                
            # å¦‚æœçœ‹èµ·ä¾†åƒè³‡æ–™å…§å®¹ï¼ˆé•·æ–‡å­—ã€æ•¸å­—ç­‰ï¼‰ï¼Œæ¸›åˆ†
            if len(text) > 20 or text.replace('.', '').replace(',', '').replace('-', '').isdigit():
                score -= 1
        
        return max(0, score)

    def _auto_detect_stock_symbol(self, chunk_df):
        """è‡ªå‹•è¾¨è­˜è‚¡ç¥¨ä»£è™Ÿå’Œä¸­æ–‡åç¨±"""
        try:
            # æª¢æŸ¥ç¬¬ä¸€åˆ—ï¼ˆè¡¨é ­ï¼‰
            first_row_symbol = self._extract_symbol_from_header(chunk_df.columns.tolist())
            if first_row_symbol:
                return first_row_symbol
            
            # å¦‚æœç¬¬ä¸€åˆ—æ²’æœ‰ï¼Œæª¢æŸ¥ç¬¬äºŒåˆ—ï¼ˆç¬¬ä¸€ç­†è³‡æ–™ï¼‰
            if len(chunk_df) > 0:
                second_row_symbol = self._extract_symbol_from_data(chunk_df.iloc[0])
                if second_row_symbol:
                    return second_row_symbol
                    
            return None
            
        except Exception as e:
            logging.error(f"è‚¡ç¥¨ä»£è™Ÿè‡ªå‹•è¾¨è­˜å¤±æ•—: {e}")
            return None

    def _ask_user_for_data_type(self, chunk_df, filename):
        """è®“ä½¿ç”¨è€…é¸æ“‡è³‡æ–™é¡å‹"""
        # é¡¯ç¤ºå‰å¹¾è¡Œè³‡æ–™è®“ä½¿ç”¨è€…ç¢ºèª
        preview = "CSVå‰3è¡Œé è¦½ï¼š\n"
        preview += f"æ¬„ä½åç¨±: {chunk_df.columns.tolist()}\n"
        if len(chunk_df) > 0:
            preview += f"ç¬¬ä¸€è¡Œè³‡æ–™: {chunk_df.iloc[0].tolist()}\n"
        if len(chunk_df) > 1:
            preview += f"ç¬¬äºŒè¡Œè³‡æ–™: {chunk_df.iloc[1].tolist()}\n"
        
        choice = simpledialog.askstring(
            "é¸æ“‡è³‡æ–™é¡å‹",
            f"{preview}\n"
            "ç„¡æ³•è‡ªå‹•åˆ¤æ–·è³‡æ–™é¡å‹ï¼Œè«‹é¸æ“‡ï¼š\n"
            "1. é¸æ“‡æ¬Š (options)\n"
            "2. æœŸè²¨ (futures)\n" 
            "3. è‚¡ç¥¨ (stocks)\n\n"
            "è«‹è¼¸å…¥é¸æ“‡ (1/2/3):"
        )
        
        if choice == '1':
            return self._import_as_options(chunk_df, filename)
        elif choice == '2':
            return self._import_as_futures(chunk_df, filename)
        elif choice == '3':
            return self._import_as_stocks(chunk_df, filename)
        else:
            return 0

    def _import_as_options(self, chunk_df, filename):
        """åŒ¯å…¥é¸æ“‡æ¬Šè³‡æ–™"""
        options_list = []
        for _, row in chunk_df.iterrows():
            options_list.append({
                'product': row.get('product', 'TXO'),
                'trade_date': row.get('trade_date', datetime.now().strftime('%Y-%m-%d')),
                'expiry': row.get('expiry', ''),
                'strike': row.get('strike', 0),
                'cp': row.get('cp', 'C'),
                'volume': row.get('volume', 0),
                'oi': row.get('oi'),
                'raw_oi_text': row.get('raw_oi_text', ''),
                'load_file': filename
            })
        
        return self.database.batch_insert_options_fast(options_list)

    def _import_as_futures(self, chunk_df, filename):
        """åŒ¯å…¥æœŸè²¨è³‡æ–™"""
        futures_list = []
        for _, row in chunk_df.iterrows():
            futures_list.append({
                'product': row.get('product', 'TXF'),
                'trade_date': row.get('trade_date', datetime.now().strftime('%Y-%m-%d')),
                'expiry': row.get('expiry', ''),
                'open': row.get('open'),
                'high': row.get('high'),
                'low': row.get('low'),
                'close': row.get('close'),
                'volume': row.get('volume', 0),
                'oi': row.get('oi', 0),
                'settlement': row.get('settlement'),
                'load_file': filename
            })
        
        return self.database.batch_insert_futures_fast(futures_list)

    def _import_as_stocks(self, chunk_df, filename):
        """åŒ¯å…¥è‚¡ç¥¨è³‡æ–™"""
        stocks_list = []
        for _, row in chunk_df.iterrows():
            stocks_list.append({
                'symbol': row.get('symbol', ''),
                'trade_date': row.get('trade_date', datetime.now().strftime('%Y-%m-%d')),
                'open': row.get('open'),
                'high': row.get('high'),
                'low': row.get('low'),
                'close': row.get('close'),
                'volume': row.get('volume', 0),
                'value': row.get('value', 0),
                'load_file': filename
            })
        
        return self.database.batch_insert_stocks_fast(stocks_list)

    def _import_as_stocks_with_symbol(self, chunk_df, filename, symbol_info):
        """ä½¿ç”¨è‡ªå‹•è¾¨è­˜çš„è‚¡ç¥¨ä»£è™ŸåŒ¯å…¥è‚¡ç¥¨è³‡æ–™"""
        stocks_list = []
        for _, row in chunk_df.iterrows():
            stocks_list.append({
                'symbol': symbol_info['symbol'],
                'chinese_name': symbol_info['chinese_name'],
                'trade_date': row.get('trade_date', datetime.now().strftime('%Y-%m-%d')),
                'open': row.get('open'),
                'high': row.get('high'),
                'low': row.get('low'),
                'close': row.get('close'),
                'volume': row.get('volume', 0),
                'value': row.get('value', 0),
                'load_file': filename
            })
        
        return self.database.batch_insert_stocks_fast(stocks_list)

    def export_database_query(self):
        """åŒ¯å‡ºè³‡æ–™åº«æŸ¥è©¢çµæœ"""
        try:
            # é¸æ“‡åŒ¯å‡ºé¡å‹
            export_type = simpledialog.askstring("åŒ¯å‡ºæŸ¥è©¢", "è«‹è¼¸å…¥æŸ¥è©¢é¡å‹ (options/futures/stocks):")
            if not export_type:
                return
                
            # åŸ·è¡ŒæŸ¥è©¢
            if export_type.lower() == 'options':
                df = self.database.query_options()
            elif export_type.lower() == 'futures':
                df = self.database.query_futures()
            elif export_type.lower() == 'stocks':
                df = self.database.query_stocks()
            else:
                messagebox.showwarning("è­¦å‘Š", "ä¸æ”¯æ´çš„æŸ¥è©¢é¡å‹")
                return
            
            # åŒ¯å‡ºæª”æ¡ˆ
            filename = f"Data/{export_type}_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            
            messagebox.showinfo("æˆåŠŸ", f"æŸ¥è©¢çµæœå·²åŒ¯å‡ºè‡³: {filename}")
            self.update_status(f"è³‡æ–™åº«æŸ¥è©¢çµæœå·²åŒ¯å‡º: {filename}")
            
        except Exception as e:
            messagebox.showerror("éŒ¯èª¤", f"åŒ¯å‡ºæŸ¥è©¢å¤±æ•—: {e}")

    def query_options(self):
        """æŸ¥è©¢é¸æ“‡æ¬Šè³‡æ–™"""
        try:
            product = simpledialog.askstring("æŸ¥è©¢é¸æ“‡æ¬Š", "å•†å“ä»£ç¢¼ (TXO/CAO/CNOï¼Œç•™ç©ºæŸ¥è©¢æ‰€æœ‰):")
            trade_date = simpledialog.askstring("æŸ¥è©¢é¸æ“‡æ¬Š", "äº¤æ˜“æ—¥æœŸ (YYYY-MM-DDï¼Œç•™ç©ºæŸ¥è©¢æ‰€æœ‰):")
            
            df = self.database.query_options(product=product, trade_date=trade_date)
            
            self.database_text.delete(1.0, tk.END)
            self.database_text.insert(tk.END, f"=== é¸æ“‡æ¬ŠæŸ¥è©¢çµæœ ===\n\n")
            self.database_text.insert(tk.END, f"æ‰¾åˆ° {len(df)} ç­†è³‡æ–™\n\n")
            self.database_text.insert(tk.END, df.to_string())
            
            self.notebook.select(3)
            self.update_status(f"é¸æ“‡æ¬ŠæŸ¥è©¢å®Œæˆ: {len(df)} ç­†è³‡æ–™")
            
        except Exception as e:
            messagebox.showerror("éŒ¯èª¤", f"æŸ¥è©¢é¸æ“‡æ¬Šå¤±æ•—: {e}")

    def query_futures(self):
        """æŸ¥è©¢æœŸè²¨è³‡æ–™"""
        try:
            product = simpledialog.askstring("æŸ¥è©¢æœŸè²¨", "å•†å“ä»£ç¢¼ (TXF/MXFï¼Œç•™ç©ºæŸ¥è©¢æ‰€æœ‰):")
            trade_date = simpledialog.askstring("æŸ¥è©¢æœŸè²¨", "äº¤æ˜“æ—¥æœŸ (YYYY-MM-DDï¼Œç•™ç©ºæŸ¥è©¢æ‰€æœ‰):")
            
            df = self.database.query_futures(product=product, trade_date=trade_date)
            
            self.database_text.delete(1.0, tk.END)
            self.database_text.insert(tk.END, f"=== æœŸè²¨æŸ¥è©¢çµæœ ===\n\n")
            self.database_text.insert(tk.END, f"æ‰¾åˆ° {len(df)} ç­†è³‡æ–™\n\n")
            self.database_text.insert(tk.END, df.to_string())
            
            self.notebook.select(3)
            self.update_status(f"æœŸè²¨æŸ¥è©¢å®Œæˆ: {len(df)} ç­†è³‡æ–™")
            
        except Exception as e:
            messagebox.showerror("éŒ¯èª¤", f"æŸ¥è©¢æœŸè²¨å¤±æ•—: {e}")

    def query_stocks(self):
        """æŸ¥è©¢è‚¡ç¥¨è³‡æ–™"""
        try:
            symbol = simpledialog.askstring("æŸ¥è©¢è‚¡ç¥¨", "è‚¡ç¥¨ä»£ç¢¼ (ç•™ç©ºæŸ¥è©¢æ‰€æœ‰):")
            trade_date = simpledialog.askstring("æŸ¥è©¢è‚¡ç¥¨", "äº¤æ˜“æ—¥æœŸ (YYYY-MM-DDï¼Œç•™ç©ºæŸ¥è©¢æ‰€æœ‰):")
            
            df = self.database.query_stocks(symbol=symbol, trade_date=trade_date)
            
            self.database_text.delete(1.0, tk.END)
            self.database_text.insert(tk.END, f"=== è‚¡ç¥¨æŸ¥è©¢çµæœ ===\n\n")
            self.database_text.insert(tk.END, f"æ‰¾åˆ° {len(df)} ç­†è³‡æ–™\n\n")
            self.database_text.insert(tk.END, df.to_string())
            
            self.notebook.select(3)
            self.update_status(f"è‚¡ç¥¨æŸ¥è©¢å®Œæˆ: {len(df)} ç­†è³‡æ–™")
            
        except Exception as e:
            messagebox.showerror("éŒ¯èª¤", f"æŸ¥è©¢è‚¡ç¥¨å¤±æ•—: {e}")

def main():
    """ä¸»ç¨‹å¼"""
    root = tk.Tk()
    app = EnhancedTXODataScraper(root)
    root.mainloop()

if __name__ == "__main__":
    main()